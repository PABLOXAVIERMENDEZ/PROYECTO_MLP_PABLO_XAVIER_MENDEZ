{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1-\tAnálisis Exploratorio: \n",
    "•\tImportamos librerías matplotlib, pandas, numpy y seaborn si bien en el documento final no hay grafica se importan para simplificar su uso en el futuro.\n",
    "•\tLectura de documento copiamos el path y utilizamos r’’ dado que mi sistema operativo es Windows y con esto no aplica las barras invertidas, también la lectura en una variable. Luego lo transformamos en en dataframe mediante mandas. Esto lo realizamos en 2 partes a modo de practica dado que en el futuro aplicaremos una sola línea de código.\n",
    "•\tEntendimiento, Visualizamos:\n",
    "Las de columnas y observaciones lo realizamos mediante Shape()\n",
    "Tipo columna, de datos y nulos lo realizamos mediante info()\n",
    "Realizamos un describe para ver el resumen estadístico de los datos.\n",
    "Cantidad de agentes, lo realizamos con unique()\n",
    "Cantidad de Motivos de llamados lo realizamos con unique()\n",
    "Horario de atención lo realizamos mediante sort value()\n",
    "\n",
    "2-\tLimpieza & Set Up de la base de datos:\n",
    "•\tLectura de documento, Volvemos a leer el documento y lo realizamos en donde incluyendo \"parse_dates\" para que la columna 'Date' se lea en el formato más adecuado y hacemos algo similar con otras 2 columnas del tipo Float que serían Speed of answer in seconds' y 'Satisfaction rating' \n",
    "•\tCambiar los títulos de las columnas a mayúsculas y agregar la leyenda del índice, para trabajar de una manera más ordenada se decide insertar un índice al cual se puede llamar en el futuro\n",
    "•\tConversión de columnas, Time & AvgTalkDuration, la columna 'Time' se modificara como tipo de dato a delta time\n",
    "La columna 'AvgTalkDuration' se modificará como tipo de dato datetime con el formato días, horas, minutos y segundos. \n",
    "Unicamente se completan los Nan de la columna con el fin de convetir el tipo de dato proximamente se analizaran por partes el Dt con el fin de analizarlo en 2 etapas del \"embudo de análisis \n",
    "•\tCreación de columna \"DAY_OF_THE_WEEK, se decide crear esta columna para visualizar de una manera más simple los días en que la empresa brinda servicio.\n",
    "Una vez creada realizamos una pivot table con el índice de los días de la semana para visualizar que días se brinda servicio.\n",
    "•\tLimpieza & generación de embudo para análisis de datos\n",
    "En primer lugar, verificamos que no figuren datos existentes en las llamadas concretadas, Una vez corroborado, procederemos separar los dt con el fin de analizaros por 2 partes distintas del embudo:\n",
    "interacciones concretadas = bbdd_Y\n",
    "interacciones no concretadas = bbdd_N\n",
    "Realizamos la verificación de nulos sobre la base de interacciones concretadas.\n",
    "Realizamos la verificación de duplicados sobre toda la base y sobre las columnas clave por ejemplo CALL ID.\n",
    "Realizamos la verificación de absurdos, recurrimos a una función que nos brinde todos los datos únicos de cada columna.\n",
    "En una segunda verificación realizamos una búsqueda de llamados fuera del horario de atención.\n",
    "•\tSe reordenan las columnas, con el fin de agilizar la visualización\n",
    "•\tSe procede a guardar las bases de datos depuradas en nuevos CSV con el fin de poder llamarla en nuestros futuros análisis y no tener que correr todas las customizaciones\n",
    "\n",
    "3.\tEstadística descriptiva & as-is de los datos:\n",
    "•\tImportamos librerías matplotlib, pandas, numpy, scipy.stats y seaborn\n",
    "•\tLectura de mi csv y se guarda en 3 variables, lectura de la base en bruto, lectura de las interacciones concretadas y lectura de las interacciones no concretadas, si bien trabajaremos principalmente con una, se realiza la lectura de las 3 con el fin de que en el caso de precisarlas ya estén cargadas y seteadas con igual formato.\n",
    "•\tConversión de columnas, Time & AvgTalkDuration, se realiza nuevamente dado que al guardar la base en un csv se vuelve a cambiar al tipo de formato más simple.\n",
    "•\tDetalle & Entendimiento de las columnas, disponibilizamos una tabla con el detalle de las columnas y a que hace referencia cada una.\n",
    "•\tAnálisis Descriptivo & Insights preliminares, calculamos la moda, la media y la mediana de los datos disponibles en Telecom\n",
    "•\tRealizamos el análisis, de cuantas llamadas fueron atendidas y cuantas fueron atendidas con un tiempo de espera menor a un minuto y cuantas fueron resueltas, establecemos variables tipo métricas para cada uno de estos análisis.\n",
    "•\tCardinalidad de los datos, realizamos el análisis de cardinalidad con el fin de ver cuántos únicos hay en cada una de las columnas cuales tienen cardinalidad alta, media y baja y proceder con el análisis univariante según el resultado, esto lo obtenemos mediante una función.\n",
    "•\tCorrelación de los datos, utilizamos un gráfico de correlación para analizar si las variables numéricas se correlacionan entre si, según la información no una relación clara entre las variables en este primer análisis.\n",
    "•\tAnálisis Univariante & distribución de las llamadas, en base a nuestro análisis de cardinalidad realizamos un análisis univariante de la distribución de llamadas, por agente, por tiempo de respuesta, por fecha, por día de la semana, llamados atendidos diariamente, semanalmente, llamados no atendidos diariamente y semanalmente, cantidad de llamados por franjas horarias de una hora y procedemos a graficarlos.\n",
    "•\tAnálisis Bivariante, En primer lugar aplicamos un pairplot para visualizar una matriz de gráficos de dispersión para visualizar las relaciones bivariadas entre diferentes variables numericas\n",
    "•\tAnálisis multivariante: analizamos 3 variables al mismo tiempo Agente, consultas resueltas o no y nivel de satisfacción con el fin de identificar si las mismas se correlacionan o no, en este análisis no logramos visualizar una correlación directa y procedemos a graficarlo.\n",
    "\n",
    "\n",
    "4.\tHipótesis \n",
    "•\tRealizamos el planteo de 5 hipótesis\n",
    "•\t1) El principal motivo de contactos representa al menos un 25% del volumen total y además este representa más del 35% del tiempo operativo insumido. Definimos las variables mayor motivo de contacto y cantidad de contacto los porcentajes que representaban y el tiempo operativo estimado, para esta hipótesis realizamos una prueba Z de proporciones.\n",
    "•\t2) La distribución de llamados siguen una distribución normal para esta hipótesis realizamos un test de shapiro para analizar la normalidad de la distribución \n",
    "•\t3) La velocidad de respuesta un 15 % más lento que el promedio  y la satisfacción del usuario afectan la probabilidad de resolución, para este test utilizamos un  análisis de regresión lineal para examinar la relación entre los niveles de satisfacción y dos variables independientes: la resolución del caso (binaria: resuelto o no resuelto) y el tiempo de espera.\n",
    "•\t4) El 70% de los casos relacionados a los pagos se dan del 25 al 05 de cada mes y son resueltos, nuevamente utilizamos una prueba Z de proporciones para realizar el test de hipótesis, filtramos la variable que buscamos por fecha y luego aplicamos la prueba de proporciones\n",
    "•\t5) De reducir un 10% de los contactos del principal motivo se generaría una eficiencia en tiempos operativos de más de un 4%, en primer lugar definimos las variables con reducción, sin reducción y lo mismo con los tiempos operativos, luego aplicamos el test t de Student independiente (asumiendo varianzas iguales, se realiza de tal manera dado que es el mismo grupo a analizar y ya se encuentra filtrado por el motivo más frecuente de llamado)\n",
    "\n",
    "\n",
    "5.\tVisualizaciones.\n",
    "Si bien se realizaron visualizaciones de los distintos análisis e hipótesis se visualizan todas juntas la ultima parte del EDA Visualizaciones \n",
    "\n",
    "_______________________________________________________________________________________________________________________\n",
    ".\tModelo predictivo de machine learning\n",
    "\n",
    "Optimización de modelos operativos en Telecom: Análisis Profundo de los contactos para Mejorar el CX.\n",
    "En este proyecto, llevaremos a cabo un análisis exhaustivo de las interacciones de los clientes con Telecom. Nuestro objetivo es descubrir insights a partir de un análisis profundo de los datos, correlacionando los mismo buscamos desarrollar conclusiones significativas, que no solo logren esclarecer  patrones de comportamiento de los clientes, sino que también proporcionen orientación para mejorar la experiencia del cliente de manera significativa.**\n",
    "El objetivo preponderante es dimensionar la demanda en volumen de llamados mediante un modelo predictivo de series temporales con una regresion lineal.\n",
    "\n",
    "1. IMPORTACIÓN DE LIBRERIAS\n",
    "\n",
    "2. IMPORTACIÓN DEL DATASET\n",
    "\n",
    "3. CREACION DE UN NUEVO DATAFRAME CON EL DETALLE DE LA CANTIDAD DE LLAMADAS POR DÍA\n",
    "\n",
    "4. GENERACIÓN DEL INDEX 'time'\n",
    "\n",
    "5. GRAFICO DE LA CANTIDAD DE LLAMADAD DIARIAS & Tendencia\n",
    "\n",
    "6. MODELO :REALIZAMOS UN MODELO DE REGRESION LINEAL, ESCOGIMOS ESTAS VARIABLES PORQUE LA CANTIDAD DE LLAMADAS DIARIAS ES UN BUEN INDICADOR DE LA CARGA DE TRABAJO DEL CENTRO DE ATENCIÓN AL CLIENTE, Y LAS FECHAS PUEDEN INCLUIR INFORMACIÓN ESTACIONAL O DE TENDENCIAS A LO LARGO DEL TIEMPO.\n",
    "\n",
    "7.AÑADIMOS UN \"LAG FEATURE\" PARA CAPTURAR LA RELACIÓN ENTRE LA CANTIDAD DE LLAMADAS EN UN DÍA Y LA CANTIDAD DE LLAMADAS EN DÍAS ANTERIORES. ESTO PUEDE SER ÚTIL PARA IDENTIFICAR PATRONES O TENDENCIAS EN EL COMPORTAMIENTO DE LAS LLAMADAS, COMO POR EJEMPLO SI HAY UN INCREMENTO O DECREMENTO CONSISTENTE EN LA CANTIDAD DE LLAMADAS DESPUÉS DE CIERTOS DÍAS O EVENTOS. EL \"LAG FEATURE\" NOS PERMITE INCORPORAR ESTA INFORMACIÓN HISTÓRICA EN NUESTRO MODELO, LO QUE PODRÍA MEJORAR SU CAPACIDAD PARA PREDECIR LA CANTIDAD DE LLAMADAS FUTURAS\n",
    "\n",
    "8. ENTRENAMOS EL MODELO Y COMPARAMOS LA PREDICCIÓN VS LOS RESULTADOS REALES.\n",
    "\n",
    "9.APLICAMOS LAS MEDIAS MÓVILES, PARA AYUDAR A SUAVIZAR LA SERIE TEMPORAL AL PROMEDIAR LOS VALORES DE UN PERÍODO ESPECÍFICO, LO QUE PUEDE AYUDAR A RESALTAR TENDENCIAS O PATRONES SUBYACENTES EN LOS DATOS AL REDUCIR EL RUIDO O LA VARIABILIDAD ALEATORIA. AL APLICAR MEDIAS MÓVILES A LA SERIE TEMPORAL, SE PUEDEN IDENTIFICAR TENDENCIAS A LARGO PLAZO QUE PODRÍAN NO SER EVIDENTES EN LOS DATOS ORIGINALES. ESTO PUEDE AYUDAR A ENTENDER MEJOR LA DIRECCIÓN GENERAL DE LOS DATOS Y A DETECTAR POSIBLES CAMBIOS EN EL COMPORTAMIENTO A LO LARGO DEL TIEMPO\n",
    "\n",
    "10.APLICAMOS STATSMODELS PARA CREAR CARACTERÍSTICAS DETERMINÍSTICAS PARA LA SERIE TEMPORAL. EL BLOQUE DE CÓDIGO QUE PROPORCIONASTE PARECE ESTAR CREANDO CARACTERÍSTICAS PARA MODELAR LA TENDENCIA Y OTROS PATRONES EN LOS DATOS DE SERIES TEMPORALES\n",
    "\n",
    "11.ENTRENAMOS EL MODELO  DE REGRESIÓN LINEAL UTILIZANDO LAS CARACTERÍSTICAS DETERMINÍSTICAS CREADAS PREVIAMENTE Y LA VARIABLE OBJETIVO, Y LUEGO REALIZA PREDICCIONES UTILIZANDO ESTE MODELO.\n",
    "\n",
    "12.GRACIFAMOS LA TENDENCIA, CON EL FIN DE IDENTIFICAR SI ES AFIN A LOS GRAFICOS ANTERIORES.\n",
    "\n",
    "_______________________________________________________________________________________________________________________\n",
    "\n",
    ". PRODUCTIVIZACIÓN\n",
    "\n",
    "Se define una aplicación web Flask que utiliza un modelo de machine learning previamente entrenado para predecir la cantidad de llamadas telefónicas en una fecha específica. La aplicación permite al usuario ingresar una fecha, un valor de constante y un valor de tendencia, y luego utiliza estos datos como entrada para realizar una predicción con el modelo. La predicción se muestra al usuario en forma de un mensaje de alerta en la interfaz web.\n",
    "\n",
    "1.Definición de la aplicación Flask: El archivo app.py define una aplicación web Flask que maneja las solicitudes HTTP y las rutas correspondientes.\n",
    "\n",
    "2.Carga del modelo de machine learning: Se carga un modelo de machine learning previamente entrenado desde un archivo modelo.pkl.\n",
    "\n",
    "3.Rutas de la aplicación: Se definen tres rutas para la aplicación: la ruta raíz '/', la ruta 'user/<name>' y la ruta '/predict'.\n",
    "\n",
    "4. Ruta raíz y plantilla HTML: La ruta raíz devuelve una plantilla HTML (index.html) que contiene un formulario para ingresar la fecha y otros valores.\n",
    "\n",
    "5.Ruta '/predict' y función de predicción: La ruta '/predict' maneja las solicitudes POST enviadas desde el formulario. Extrae los datos ingresados por el usuario, realiza una predicción utilizando el modelo de machine learning y devuelve el resultado como texto.\n",
    "\n",
    "6.Interfaz web y JavaScript: La interfaz web (index.html) contiene un formulario para ingresar los datos y un script de JavaScript que captura los datos ingresados por el usuario, los envía al servidor Flask a través de una solicitud POST y muestra la respuesta de la predicción en un mensaje de alerta.\n",
    "\n",
    "7.Creación de un docker file con la instalación de dependencias: Se actualizan e instalan las dependencias necesarias, como el intérprete de Python y las herramientas de desarrollo.\n",
    "\n",
    "8.Copia de la aplicación: Se copia el código de la aplicación Flask al directorio de trabajo en el contenedor.\n",
    "\n",
    "9.Configuración de exposición de puerto y comando por defecto: Se establece el directorio de trabajo, se expone el puerto 5000 para acceder a la aplicación y se define el comando por defecto para ejecutar la aplicación Flask.\n",
    "\n",
    "10.Este Dockerfile proporciona un conjunto claro y conciso de instrucciones para crear y ejecutar el contenedor Docker que aloja la aplicación Flask.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
